{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for Diabetes\n",
    "\n",
    "\n",
    "This notebook proposes a reinforcement-learning based algorithm for optimal control of blood glucose in patients with type-1 diabetes. Specifically, the algorithm aims to suggest an optimal insulin injection policy. Its performance was assessed using simulations on a combination of the minimum model and part of the Hovorka model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train DDPG to learn optimal insulin policy\n",
    "\n",
    "**Deep Deterministic Policy Gradients(DDPG)[1]**\n",
    "\n",
    "Reference:\n",
    "- [1] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015, September 9). CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\n",
    ". arXiv.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=cuda4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/simglucose/lib/python3.6/site-packages/theano/gpuarray/dnn.py:184: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to a version >= v5 and <= v7.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 7401 on context None\n",
      "Preallocating 10869/11441 Mb (0.950000) on cuda4\n",
      "Mapped name None to device cuda4: Tesla K80 (0000:00:1B.0)\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='device=cuda4'\n",
    "import theano\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import gym\n",
    "\n",
    "from garage.envs.normalized_env import normalize\n",
    "from garage.exploration_strategies.ou_strategy import OUStrategy\n",
    "from garage.replay_buffer import SimpleReplayBuffer\n",
    "from gym.envs.registration import register\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adolescent#001\n",
      "Dexcom\n",
      "Insulet\n",
      "[(7, 45), (12, 70), (16, 15), (18, 80), (23, 10)]\n"
     ]
    }
   ],
   "source": [
    "import gym_envs.simglucose_env\n",
    "env = gym.make('simglucose-adolescent2-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the environment \n",
    "env = normalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garage.theano.algos import DDPG\n",
    "from garage.theano.policies import DeterministicMLPPolicy\n",
    "from garage.theano.q_functions import ContinuousMLPQFunction\n",
    "from garage.theano.envs import TheanoEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07 12:21:38 | Populating workers...\n",
      "2018-12-07 12:21:38 | Populated\n",
      "2018-12-07 12:21:43 | epoch #0 | Training started\n"
     ]
    }
   ],
   "source": [
    "#Returns a Theano wrapper class for gym.Env.\n",
    "theano_env = TheanoEnv(env)\n",
    "#define policy network(A policy function, controls how our agent acts.)\n",
    "policy = DeterministicMLPPolicy(\n",
    "    env_spec=theano_env.spec,\n",
    "    # The neural network policy should have two hidden layers, each with 32 hidden units.\n",
    "    hidden_sizes=(32, 32)\n",
    ")\n",
    "\n",
    "# define exploration strategy\n",
    "es = OUStrategy(env_spec=theano_env.spec)\n",
    "\n",
    "\n",
    "#define critic network(A value function, measures how good these actions are.)\n",
    "qf = ContinuousMLPQFunction(env_spec=theano_env.spec)\n",
    "\n",
    "#replay buffer breaks temporal correlations and thus benefits RL algorithms.\n",
    "replay_buffer = SimpleReplayBuffer(\n",
    "        env_spec=theano_env.spec, size_in_transitions=int(1e6), time_horizon=100)\n",
    "\n",
    "# uses DDPG algo from garage\n",
    "algo = DDPG(\n",
    "    env=theano_env,\n",
    "    policy=policy,\n",
    "    qf=qf,\n",
    "    es=es,\n",
    "    pool=replay_buffer,\n",
    "    batch_size=32,\n",
    "    max_path_length=100,\n",
    "    epoch_length=1000,\n",
    "    min_pool_size=10000,\n",
    "    n_epochs=NUM_EPOCH,\n",
    "    discount=0.99,\n",
    "    scale_reward=0.01,\n",
    "    qf_learning_rate=1e-3,#critic learning rate\n",
    "    policy_learning_rate=1e-4,#actor learning rate\n",
    "    #plot=True,\n",
    "    #pause_for_plot=True\n",
    "    #evaluate=True\n",
    ")\n",
    "\n",
    "# Set up logger since we are not using run_experiment\n",
    "# tabular_log_file = osp.join(log_dir, \"progress.csv\")\n",
    "# garage_logger.add_tabular_output(tabular_log_file)\n",
    "# garage_logger.set_tensorboard_dir(log_dir)\n",
    "    \n",
    "# trains the model\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save DDPG Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "ts = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "pickle.dump(algo.policy,open(f\"../output/ddpg_policy_{ts}\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement the Insulin controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DDPG_POLICY = \"../output/ddpg_policy_2018-12-07-03-53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simglucose.simulation.user_interface import simulate\n",
    "from simglucose.controller.base import Controller, Action\n",
    "\n",
    "\n",
    "class DDPGController(Controller):\n",
    "    def __init__(self, init_state):\n",
    "        self.init_state = init_state\n",
    "        self.state = init_state\n",
    "        self.model = pickle.load(open(SAVED_DDPG_POLICY,\"rb\"))\n",
    "\n",
    "    def policy(self, observation, reward, done, **info):\n",
    "        '''\n",
    "        Every controller must have this implementation!\n",
    "        ----\n",
    "        Inputs:\n",
    "        observation - a namedtuple defined in simglucose.simulation.env. For\n",
    "                      now, it only has one entry: blood glucose level measured\n",
    "                      by CGM sensor.\n",
    "        reward      - current reward returned by environment\n",
    "        done        - True, game over. False, game continues\n",
    "        info        - additional information as key word arguments,\n",
    "                      simglucose.simulation.env.T1DSimEnv returns patient_name\n",
    "                      and sample_time\n",
    "        ----\n",
    "        Output:\n",
    "        action - a namedtuple defined at the beginning of this file. The\n",
    "                 controller action contains two entries: basal, bolus\n",
    "        '''\n",
    "        self.state = observation\n",
    "        \n",
    "        # Action in the gym environment is a scalar\n",
    "        # representing the basal insulin, which differs from\n",
    "        # the regular controller action outside the gym\n",
    "        # environment (a tuple (basal, bolus)).\n",
    "        # In the perfect situation, the agent should be able\n",
    "        # to control the glucose only through basal instead\n",
    "        # of asking patient to take bolus\n",
    "        # so i'm setting bolus to None and only return basal here\n",
    "    \n",
    "        basal = self.model.get_action(observation)[0][0]\n",
    "        bolus = None\n",
    "        action = basal\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset the controller state to inital state, must be implemented\n",
    "        '''\n",
    "        self.state = self.init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('simglucose-adolescent2-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### basal_bolus controller (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simglucose.controller.basal_bolus_ctrller import BBController\n",
    "bb_ctrller = BBController()\n",
    "\n",
    "reward = 0\n",
    "done = False\n",
    "info = {'sample_time': 3,\n",
    "        'patient_name': 'adolescent#002',\n",
    "        'meal': 0}\n",
    "\n",
    "observation = env.reset()\n",
    "for t in range(T_STEP):\n",
    "    env.render(mode='human')\n",
    "    #print(observation)\n",
    "    # action = env.action_space.sample()\n",
    "    ctrl_action = bb_ctrller.policy(observation, reward, done, **info)\n",
    "    action = ctrl_action.basal + ctrl_action.bolus\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RL DDPG controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddpg_ctrller = DDPGController(0)\n",
    "\n",
    "observation =  test_env.reset()\n",
    "for t in range(T_STEP):\n",
    "    test_env.render(mode='human')\n",
    "    action = ddpg_ctrller.policy(observation, reward, done, **info)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:simglucose]",
   "language": "python",
   "name": "conda-env-simglucose-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
